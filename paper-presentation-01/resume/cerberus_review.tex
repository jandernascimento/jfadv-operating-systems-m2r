\documentclass[journal]{IEEEtran}

\usepackage{color}
\usepackage{listings}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{subfig}
\usepackage{mdwlist}
\usepackage[pagebackref=true]{hyperref}%

\hyphenation{hetero-geneous op-tical net-works semi-conduc-tor}


\begin{document}
%
\title{A Case for Scaling Applications to Many-core with OS Clustering}
\author{
(reviewed by Oleg Iegorov and Jander Nascimento)}
% type author(s) between braces
\maketitle

%\section{Introduction}
This paper proposes a new way to scale operating systems for many cores
--- \emph{OS clustering}. 
%The need to scale an OS in many-core environments
%stems from the fact, that with the increasing number of cores, OS should
%efficiently share them while avoiding data contention. 
The need to scale
stems from the fact, that with the increasing number of cores in
contemporary systems, OS should
efficiently share these cores between running applications, while avoiding data contention. 
The proposed
system called \emph{Cerberus} clusters several commodity operating systems atop
a VMM and provides to applications the traditional shared memory
interface.  Cerberus adds to VMM the support for resource sharing and
communication between operating systems (running as VMs), as well as implementing system
call routing to provide an illusion for applications as running on a
single operating system.  The motivation of this approach is that
commodity operating systems (Linux, FreeBSD, etc.) can scale well with a
small number of CPU cores, while one VMM can efficiently consolidate
multiple operating systems. Thus Cerberus's approach uses several
operating systems to serve one application, without requiring any
modification of existing parallel programs.

The problem of scaling operating systems on shared memory multicore and
multiprocessor machines is not new, and there basically exist two main
approaches to solve this problem:

\begin{enumerate*}
  \item designing new operating systems from scratch;
  \item refining commodity OS kernels.
\end{enumerate*}

Cerberus uses a middle ground between these two trends, taking advantage
of both solutions' ideas.

The first main idea used by Cerberus is an extension of traditional VMMs
with support for efficient resource sharing among the clustered
operating systems. This extension is needed due to the fact that
different OS may want to share some resources (address spaces, files), as
one application can run on several OS. However, traditional VMMs (like
Xen) were designed to separate as much as possible running VMs. Solution
to this problem used by Cerberus is an addition of address range support
to VMM, as well as an efficient distributed file system.

The second idea consists in incorporation of a system call
virtualization layer. This allows processes/threads of one application
to be executed in multiple operating systems, providing users with the
illusion of running in a single OS. This layer uses the notion of
``SuperProcess'', which groups processes/threads in multiple OS to
manage the spawned processes/threads.

\section{Overview of Cerberus}

As was metioned before, one of the main contributions of Cerberus is the
modification of VMM to allow one application run on several VMs
simultaneously. This means that processes/threads belonging to one
shared-memory application now run on multiple operating systems. To
ensure system consistency in these conditions Cerberus uses the
following mechanisms.

\begin{itemize}
  \item Single Shared Memory Interface.

    Cerberus does not require modification of implemented parallel
    applications, and thus should provide to them the existing
    shared-memory interface (e.g. POSIX API). To address this issue,
    Cerberus incorporates a system-call virtualization layer which
    coordinates system calls in multiple clustered operating systems.
    All the processes/threads running on different OSes are logically
    grouped by a \emph{SuperProcess} entity.

  \item Efficient Resource Sharing.

   As traditional VMMs are designed to isolate VMs from each other, the
   other issue addressed by Cerberus was to allow an efficient sharing
   of resources (files, networking, etc.) among processes/threads. Thus,
   Cerberus implements a resource-sharing layer in both the VMM and
   operating systems.

	\section{Process spawning} 
	
	Since in Cerberus we are talking about several operating system running a single application, this would spawn several processes, but the semantics of single application the idea of \emph{super process} is used to unify the different spawned process along those different machines.
	
	Spawning a process in a different operating system first requires that the OS create a new process it-self, and then retrieves the last \emph{checkpoint} stores in the \emph{share memory}. 
	
	It is for certain heavier to create several threads remotely than locally, but Cerberus offers a mechanism to reduced by using the \emph{parallel fork / clone} which allows to create simultaneously processes/threads in different VMs.	
	
	Maintaining the consistency between the processes (\emph{subprocesses}) is done by propagating the changes in the \emph{memory mapping} of one OS the the other OSes, which is achieved by replicating mapping and unmapping request.
	
	The entry point to allow Cerberus to interact and manage process without design a new operating system is to intercept \textit{system calls} that manipulates the process informations, such as identity, creating, killing, process affinity. So the system call is intercepted, the arguments are translated by Cerberus and then the operation is dispatched to the required OS. %marshals before returning, how to say that?
	
	Since the processes created is differents OS - other than the one that requested the process creation - are completely segregated by the OSes, a mechanism should be adopted to keep the relation ship among then, the solution is to keep a global mapping of the \emph{virtual process ID} (pid abstraction created by Cerberus) and use this ID to record the relationship.
	
	So several other informations are kept in a shared manner like a \emph{page table}, \emph{inode table}. But in order not too overload the communication channel with unnecessary message a \emph{hierarchical massage-passing mechanism} is used.
	
	\section{Fallback mechanism / Resource sharing}
	
	In an example of a multi-threaded application, a memory sector is shared among those threads (which does not happen in case of forked process, where the relationship dependency is minimal), Cerberus share this memory sector by maintaining a global list of \emph{shared address ranges}.
	
	%should talk about pagefault in here
	
	Sharing efficiently the page table is a keypoint, to provided that the \emph{address range abstraction} is used to share the page table by using multiple levels, that are identified by the address range. The level of sharing which should be adopted is chosen by the size of the address range.
	
	%{talk about it subprocess} 
	
	\section{File sharing}
	
	Nowadays, the file sharing mechanisms, specifically NFS, add an undesired contention in the network, first for been centralized and secondly by adding overhead for using the network file system. %input the value of how slow is NFS compared with other filesystems?
	
	Fortunately most of multi-threaded application required only exclusive access to those resources, but for those where this is a requirement a hybrid approach is used. 

	So by combining local and remote file access is possible to reduce the contention while maintain an acceptable performance file sharing. %what is acceptable performance file sharing???
	
	By creating an NFS-like interface application per say, the sharing mechanism allows to access a file directly when it is available locally, and request for the \emph{CFS Server} when the file is only available remotely.

	Since the access hides the complexity of having several OSes in the backend, this creates a problem for a simple list operation, which would required to dispatch the listing information to every OS involved and wait for the results before return to the user.
	
	Creating virtual devices and scale this virtual devices with a set of real hardware is another possibility that allows to reduce the contention by using multiple hardwares and balance the virtual hardware call. This is possible thanks to the Virtualization environment. One example is the NIC, which can be related not only to one physical device, but several. 
	
	\section{Prototyping}
	
	Cerberus was implemented based on Xen shadow mode page table management. To reduce the number of changes required without giving as much as possible support for already implemented application, the POSIX implementation was changed, but only a subset of it, enough to support MapReduce applications, MemCached and benchmarks for filesystems.
	
	\subsection{Message passing}
	
	Cerberus create automatically in every OSes, present in the cluster, a point-to-point communication channel among every VM. The events are put in a queue and after that they can be processed in FIFO order.
	
	%talk about crash recovery model?
	
	\subsection{Memory management}
	
	For sharing page table, a scheme called Xen direct mode was implemented - which supports paravirtualization as well - but this required much more changes in the guest OS. As this implementation show to be not efficient the it was used shadow page tables and P2M instead.
	
	Before a VM handles system calls related to memory management, it will be forced to synchronize with the other VMs.
	
	\subsection{Cerberos file system}
	
	The control of the file system is kept based on local inodes, remote inodes and dentry. Local inode represents the file that can be accessed directly, which is located in the same VM domain of the requester. Remove inode receives the owner domain, doing so, the VM is capable to tell where should request this file from. Dentry is an organization inode, which can agregate several inodes.

	\subsection{System call virtualization}

	Two types of system calls must be virtualized: local and global state information access.
	
	The local state access does not offer any problem nor require any intervention by Cerberus, but the ones that modify a global state of the system need to be translated by Cerberus.
	
	Only 35 System calls from POSIX had to be implemented. In total 1800 lines of code were added to Xen, plus a kernel module of 8800 lines of code %35 of how many!?
	
	\section{Results}
	
	By the ping pong is possible to check that virtualization adds some overhead to the mechanism, but even though it is faster send a signal across VMs then sending local signals. This happens for two reasons.
	\begin{itemize}
	\item The inter-VM messaging passing is efficient
	\item signaling the target process and execute the sender can be done in parallel
	\end{itemize}
	
\end{itemize}



\end{document}

